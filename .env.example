# ============================================================================
# Agentic Contextualizer — Configuration Reference
# ============================================================================
# Copy this file to .env and fill in your values.
# All settings have sensible defaults — only API keys are required.

# ─── LLM Provider ───────────────────────────────────────────────────────────
# Which provider backend to use.
#   "anthropic" — Direct Anthropic API (default)
#   "litellm"   — LiteLLM proxy / custom gateways (recommended for gateways)
LLM_PROVIDER=anthropic

# Model identifier. Format depends on provider:
#   Direct:  claude-sonnet-4-5-20250929, gpt-4o, gemini-1.5-pro
#   LiteLLM: any model string your gateway supports
MODEL_NAME=claude-sonnet-4-5-20250929

# ─── API Keys ───────────────────────────────────────────────────────────────
# Set the key for your chosen provider. When using a gateway (LLM_BASE_URL),
# any single key is sufficient — the gateway handles routing.
ANTHROPIC_API_KEY=your_api_key_here
# OPENAI_API_KEY=sk-...
# GOOGLE_API_KEY=...

# ─── Custom Endpoint / Gateway ──────────────────────────────────────────────
# For LiteLLM proxy, OpenRouter, or any custom gateway.
# When set, LLM_PROVIDER is automatically treated as "litellm".
# Backward compatibility: ANTHROPIC_BASE_URL still works if LLM_BASE_URL is not set.
#
# Examples:
#   LiteLLM proxy:      https://litellm.example.com
#   OpenRouter:          https://openrouter.ai/api/v1
#   Local LM Studio:    http://localhost:1234/v1
#   Predibase Gateway:  https://aigateways.staging.predibase.com/xxx/router
#
# LLM_BASE_URL=https://your-gateway.example.com

# ─── LLM Connection ────────────────────────────────────────────────────────
# Max retry attempts on transient errors (default: 3)
LLM_MAX_RETRIES=3
# Request timeout in seconds (default: 60)
LLM_TIMEOUT=60

# ─── Rate Limiting — TPM-Aware ───────────────────────────────────────────
# Controls token-per-minute budget to avoid 429 rate limit errors.
# Tune these for your provider tier (e.g., Anthropic free tier = 30K TPM).
#
# MAX_TPM: Your provider's tokens-per-minute limit (default: 30000)
#   Check your provider dashboard for exact limits.
MAX_TPM=30000
#
# TPM_SAFETY_FACTOR: Use only this fraction of MAX_TPM proactively (default: 0.85)
#   Reserves buffer for token estimation errors (~10-15% on Claude/Gemini).
TPM_SAFETY_FACTOR=0.85
#
# MAX_TOKENS_PER_CALL: Reject any single LLM call estimated above this (default: unset)
#   Set to prevent accidentally sending oversized requests.
# MAX_TOKENS_PER_CALL=8000
#
# RETRY_MAX_ATTEMPTS: How many times to retry on 429 errors (default: 3)
RETRY_MAX_ATTEMPTS=3
#
# RETRY_INITIAL_WAIT: Initial backoff in seconds, doubles each retry (default: 2.0)
RETRY_INITIAL_WAIT=2.0

# ─── Token Budget ──────────────────────────────────────────────────────────
# These settings control token usage to stay within rate limits and manage cost.
#
# LLM_MAX_OUTPUT_TOKENS: Cap tokens generated per LLM call (default: 4096)
#   Lower to reduce cost and stay within rate limits (e.g., 2048)
LLM_MAX_OUTPUT_TOKENS=4096
#
# LLM_MAX_INPUT_TOKENS: Cap total input tokens per LLM call (default: unset = no limit)
#   Trims old messages to stay under this limit, keeping the most recent context.
#   Set to ~80% of your gateway's per-minute token limit (e.g., 20000 for a 30k limit).
#   Uses approximate counting (~4 chars per token).
LLM_MAX_INPUT_TOKENS=20000
#
# MAX_TOOL_OUTPUT_CHARS: Truncate any single tool response beyond this (default: 12000)
#   Prevents a single tool call from bloating the conversation history.
#   Lower for tight budgets (e.g., 6000), higher for detailed analysis (e.g., 20000)
MAX_TOOL_OUTPUT_CHARS=12000
#
# MAX_SCAN_FILES: How many files scan_structure returns (default: 200)
#   Lower for large repos or tight budgets (e.g., 50)
MAX_SCAN_FILES=200

# ─── Scanner Settings ──────────────────────────────────────────────────────
# MAX_FILE_SIZE: Maximum file size in bytes to read (default: 1000000 = 1MB)
#   Files larger than this are skipped during scanning.
MAX_FILE_SIZE=1000000
#
# IGNORED_DIRS: Comma-separated list of additional directories to skip.
#   These are added to the built-in ignore list:
#   .git, node_modules, __pycache__, .venv, venv, dist, build, .pytest_cache
# IGNORED_DIRS=.mypy_cache,.tox,coverage

# ─── Output Settings ───────────────────────────────────────────────────────
# OUTPUT_DIR: Where generated context files are saved (default: contexts)
# OUTPUT_DIR=contexts
