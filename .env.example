# ============================================================================
# Agentic Contextualizer — Configuration Reference
# ============================================================================
# Copy this file to .env and fill in your values.
# All settings have sensible defaults — only API keys are required.

# ─── LLM Provider ───────────────────────────────────────────────────────────
# Which provider backend to use.
#   "anthropic" — Direct Anthropic API (default)
#   "litellm"   — LiteLLM proxy / custom gateways (recommended for gateways)
LLM_PROVIDER=anthropic

# Model identifier. Format depends on provider:
#   Direct:  claude-sonnet-4-5-20250929, gpt-4o, gemini-1.5-pro
#   LiteLLM: any model string your gateway supports
MODEL_NAME=claude-sonnet-4-5-20250929

# ─── API Keys ───────────────────────────────────────────────────────────────
# Set the key for your chosen provider. When using a gateway (LLM_BASE_URL),
# any single key is sufficient — the gateway handles routing.
ANTHROPIC_API_KEY=your_api_key_here
# OPENAI_API_KEY=sk-...
# GOOGLE_API_KEY=...

# ─── Custom Endpoint / Gateway ──────────────────────────────────────────────
# For LiteLLM proxy, OpenRouter, or any custom gateway.
# When set, LLM_PROVIDER is automatically treated as "litellm".
# Backward compatibility: ANTHROPIC_BASE_URL still works if LLM_BASE_URL is not set.
#
# Examples:
#   LiteLLM proxy:      https://litellm.example.com
#   OpenRouter:          https://openrouter.ai/api/v1
#   Local LM Studio:    http://localhost:1234/v1
#   Predibase Gateway:  https://aigateways.staging.predibase.com/xxx/router
#
# LLM_BASE_URL=https://your-gateway.example.com

# ─── LLM Connection ────────────────────────────────────────────────────────
# Max retry attempts on transient errors (default: 3)
LLM_MAX_RETRIES=3
# Request timeout in seconds (default: 60)
LLM_TIMEOUT=60

# ─── Rate Limiting (Agent Mode) ────────────────────────────────────────────
# Controls how fast the agent makes LLM calls. Tune for your gateway's limits.
#
# RATE_LIMIT_RPS: Requests per second (default: 0.3 = ~1 call every 3s)
#   Lower if hitting token-per-minute limits (e.g., 0.1 = 1 call every 10s)
#   Higher for generous limits (e.g., 1.0 = 1 call per second)
RATE_LIMIT_RPS=0.3
#
# RATE_LIMIT_BURST: Max burst of back-to-back calls before throttling (default: 3)
RATE_LIMIT_BURST=3

# ─── Token Budget ──────────────────────────────────────────────────────────
# These settings control token usage to stay within rate limits and manage cost.
#
# LLM_MAX_OUTPUT_TOKENS: Cap tokens generated per LLM call (default: 4096)
#   Lower to reduce cost and stay within rate limits (e.g., 2048)
LLM_MAX_OUTPUT_TOKENS=4096
#
# LLM_MAX_INPUT_TOKENS: Cap total input tokens per LLM call (default: unset = no limit)
#   Trims old messages to stay under this limit, keeping the most recent context.
#   Set to ~80% of your gateway's per-minute token limit (e.g., 20000 for a 30k limit).
#   Uses approximate counting (~4 chars per token).
LLM_MAX_INPUT_TOKENS=20000
#
# MAX_TOOL_OUTPUT_CHARS: Truncate any single tool response beyond this (default: 12000)
#   Prevents a single tool call from bloating the conversation history.
#   Lower for tight budgets (e.g., 6000), higher for detailed analysis (e.g., 20000)
MAX_TOOL_OUTPUT_CHARS=12000
#
# MAX_SCAN_FILES: How many files scan_structure returns (default: 200)
#   Lower for large repos or tight budgets (e.g., 50)
MAX_SCAN_FILES=200

# ─── Scanner Settings ──────────────────────────────────────────────────────
# MAX_FILE_SIZE: Maximum file size in bytes to read (default: 1000000 = 1MB)
#   Files larger than this are skipped during scanning.
MAX_FILE_SIZE=1000000
#
# IGNORED_DIRS: Comma-separated list of additional directories to skip.
#   These are added to the built-in ignore list:
#   .git, node_modules, __pycache__, .venv, venv, dist, build, .pytest_cache
# IGNORED_DIRS=.mypy_cache,.tox,coverage

# ─── Output Settings ───────────────────────────────────────────────────────
# OUTPUT_DIR: Where generated context files are saved (default: contexts)
# OUTPUT_DIR=contexts
